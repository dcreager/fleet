.TH "flt_run" "3" "2014-01-01" "Fleet" "Fleet\ documentation"
.SH NAME
.PP
flt_run \[en] Scheduling tasks
.SH SYNOPSIS
.PP
\f[B]#include <fleet.h>\f[]
.PP
void
.PD 0
.P
.PD
\f[B]flt_run\f[](struct flt *\f[I]flt\f[], flt_task *\f[I]task\f[],
.PD 0
.P
.PD
\ \ \ \ \ \ \ \ void *\f[I]u1\f[], void *\f[I]u2\f[], void *\f[I]u3\f[],
void *\f[I]u4\f[]);
.PP
void
.PD 0
.P
.PD
\f[B]flt_run_later\f[](struct flt *\f[I]flt\f[], flt_task
*\f[I]task\f[],
.PD 0
.P
.PD
\ \ \ \ \ \ \ \ \ \ \ \ \ \ void *\f[I]u1\f[], void *\f[I]u2\f[], void
*\f[I]u3\f[], void *\f[I]u4\f[]);
.PP
void
.PD 0
.P
.PD
\f[B]flt_return_to\f[](struct flt *\f[I]flt\f[], flt_task
*\f[I]task\f[],
.PD 0
.P
.PD
\ \ \ \ \ \ \ \ \ \ \ \ \ \ void *\f[I]u1\f[], void *\f[I]u2\f[], void
*\f[I]u3\f[], void *\f[I]u4\f[]);
.PP
void
.PD 0
.P
.PD
\f[B]flt_then\f[](struct flt *\f[I]flt\f[],
.PD 0
.P
.PD
\ \ \ \ \ \ \ \ \ flt_task *\f[I]first\f[], void *\f[I]fu1\f[], void
*\f[I]fu2\f[], void *\f[I]fu3\f[], void *\f[I]fu4\f[],
.PD 0
.P
.PD
\ \ \ \ \ \ \ \ \ flt_task *\f[I]second\f[], void *\f[I]su1\f[], void
*\f[I]su2\f[], void *\f[I]su3\f[], void *\f[I]su4\f[]);
.SH DESCRIPTION
.PP
The \f[B]flt_run\f[] family of functions let you schedule a new task to
be run.
All of these functions take a \f[B]flt\f[](3) instance as their first
parameter.
However, there's no way to allocate your own \f[B]flt\f[](3) instance;
you only obtain one when a task function is executed.
That means that these functions can only be called from within an
already running task.
(Use \f[B]flt_fleet_run\f[](3) to start the initial \[lq]root\[rq] task
for a fleet.)
.PP
\f[B]flt_run\f[]() is the simplest function.
It gives you no control over when or where the new task is executed;
however, this gives the underlying fleet the most flexibility in
executing tasks in parallel.
You should only use the more complex variants if you really need the
extra constraints that they impose.
.PP
\f[B]flt_run_later\f[]() works just like \f[B]flt_run\f[](), but
provides a hint to the scheduler that it should allow other existing
tasks to run first.
.PP
\f[B]flt_return_to\f[]() is a more efficient version of
\f[B]flt_run\f[]() that can only be used as the last statement of a
parent task.
(It can be anywhere in the parent task's function body, but must be used
in a \f[C]return\f[] statement to indicate that it's the last statement
the parent task will execute.) For example:
.IP
.nf
\f[C]
static\ flt_task\ \ child_task;
static\ flt_task\ \ parent_task;

static\ void
parent_task(struct\ flt\ *flt,\ void\ *u1,\ void\ *u2,\ void\ *u3,\ void\ *u4)
{
\ \ \ \ if\ (child_should_handle_work_instead)\ {
\ \ \ \ \ \ \ \ return\ flt_return_to(flt,\ child_task,\ u1,\ u2,\ u3,\ u4);
\ \ \ \ }\ else\ {
\ \ \ \ \ \ \ \ /*\ do\ some\ work\ */
\ \ \ \ }
}
\f[]
.fi
.PP
On some platforms, this will automatically pass control to the child
task without passing through the fleet's scheduler first.
However, this is an optional optimization; like with \f[B]flt_run\f[](),
we don't make any guarantees about when or where the child task will be
executed.
.PP
\f[B]flt_then\f[]() schedules both \f[I]first\f[] and \f[I]second\f[],
but doesn't allow \f[I]second\f[] to execute until \f[I]first\f[]
finishes.
This is your primary low\-level mechanism for enforcing \[lq]executes
after\[rq] relationships between tasks.
.PP
For all of these functions, you must ensure that any \f[I]uX\f[] input
parameters remain valid until the task has a chance to run.
Effectively, this means that the responsibility for freeing the inputs
belongs to the new task, or to some subtask that it creates.
.PP
Note that all of these functions are allowed to return before the newly
scheduled child task starts to execute.
There isn't any way for the parent task to wait for newly scheduled
child tasks to start or complete.
If you need to know when the child task completes, you must use
\f[B]flt_then\f[]() to schedule a \f[I]second\f[] child task, which will
be executed when the first one finishes.
.SS Data sharing
.PP
We don't place any restrictions on which particular values you pass in
for the \f[I]uX\f[] parameters for your tasks.
However, for the most part, we also don't make any guarantees about when
or where a particular task is going to run.
That means that if you pass a pointer to a shared data instance to two
tasks, it's entirely possible for those two tasks to execute
simultaneously, in separate native OS threads.
.PP
If the tasks only \f[B]read\f[] from the shared data, then everything is
fine.
The shared data is never changed or updated, and so it doesn't matter
when or where each task reads the data.
(The only wrinkle is that you must ensure that the data isn't freed
until all of the tasks that use the data have finished.)
.PP
If one or more of the tasks need to \f[B]write\f[] to the shared data,
the story is different.
You must ensure that it's not possible for there to be any read/write or
write/write conflicts.
Fleet does not provide any portable thread synchronization primitives,
so there are two ways you can accomplish this.
.PP
The first is to use \f[B]flt_return_to\f[]() to schedule a new task.
This function must be the last statement the parent task executes, so
it's safe for the parent to pass any of its \f[I]uX\f[] parameters on to
the new child task: anything the parent does to the data must happen
\f[I]before\f[] the \f[B]flt_return_to\f[]() call, and everything the
child does to the data must happen \f[I]after\f[].
So this data sharing pattern is safe.
.PP
The other exception is when you use \f[B]flt_then\f[]() to ensure that a
\f[I]first\f[] task finishes before a \f[I]second\f[] task starts.
In this case, it's safe to share data between those two tasks, since the
scheduler guarantees that the two tasks can't possibly execute at the
same time, and so they can't possibly access the shared data
simultaneously.
